export class Agent {
    constructor(ui) {
        this.ui = ui;
        this.groqApiKey = null;
        this.currentModel = 'raciocinio';
        this.groqUrl = 'https://api.groq.com/openai/v1/chat/completions';
        this.conversationHistory = [];
        this.maxHistoryMessages = 50;
        this.abortController = null;
        this.isGenerating = false;
    }

    setModel(model) {
        this.currentModel = model;
    }

    getGroqApiKey() {
        if (!this.groqApiKey) {
            this.groqApiKey = localStorage.getItem('groq_api_key');
        }
        return this.groqApiKey;
    }

    async processMessage(userMessage, attachedFilesFromUI = null) {
        console.log('üì® Mensagem para processar:', userMessage.substring(0, 100) + '...');
        console.log('üì® Tamanho total:', userMessage.length, 'caracteres');

        // Se a UI passou arquivos explicitamente, priorizamos esses (m√°x 3)
        let parsedFiles = [];
        if (attachedFilesFromUI && Array.isArray(attachedFilesFromUI) && attachedFilesFromUI.length > 0) {
            parsedFiles = attachedFilesFromUI.slice(0, 3).map(f => ({ name: f.name, content: (f.content == null) ? '' : String(f.content) }));
            console.log('üìÅ Arquivos recebidos diretamente da UI:', parsedFiles.map(f => `${f.name} (${(f.content||'').length} chars)`));
            const emptyFiles = parsedFiles.filter(f => !f.content || f.content.trim().length === 0);
            if (emptyFiles.length > 0) {
                const names = emptyFiles.map(f => f.name).join(', ');
                const warning = `‚ùó Alguns arquivos anexados est√£o vazios ou n√£o foram salvos corretamente: ${names}. Por favor, verifique os arquivos.`;
                console.warn(warning);
                this.ui.addAssistantMessage(warning);
                return; // Bloquear processamento
            }
            // Preparar blocos para envio ao modelo
            this.lastParsedFiles = parsedFiles;
            this.extraMessagesForNextCall = [{ role: 'system', content: parsedFiles.map(f => `---FILE: ${f.name}---\n${f.content}\n---END FILE---`).join('\n\n') }];
            console.log('‚û°Ô∏è Arquivos anexados preparados para envio:', parsedFiles.map(f => f.name).join(', '));
            this.useMistralForThisMessage = true;
        } else {
            this.lastParsedFiles = [];
            this.extraMessagesForNextCall = null;
            this.useMistralForThisMessage = false;

            // Se n√£o h√° anexos do usu√°rio, verificar se o chat tem arquivos gerados anteriormente pelo assistente (para reutiliza√ß√£o)
            try {
                const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
                if (chat && chat.generatedFiles && chat.generatedFiles.length > 0) {
                    this.lastParsedFiles = chat.generatedFiles.slice(0, 3).map(f => ({ name: f.name, content: f.content }));
                    this.extraMessagesForNextCall = [{ role: 'system', content: this.lastParsedFiles.map(f => `---FILE: ${f.name}---\n${f.content}\n---END FILE---`).join('\n\n') }];
                    console.log('‚ôªÔ∏è Reusando arquivos gerados pelo assistente do chat para pr√≥xima chamada:', this.lastParsedFiles.map(f => f.name));
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Erro verificando arquivos gerados do chat:', e);
            }
        }

        this.isGenerating = true;
        this.ui.updateSendButtonToPause();
        
        if (this.currentModel === 'rapido') {
            await this.processRapidoModel(userMessage);
        } else if (this.currentModel === 'raciocinio') {
            if (this.useMistralForThisMessage) {
                await this.processMistralModel(userMessage);
            } else {
                await this.processRaciocioModel(userMessage);
            }
        } else if (this.currentModel === 'pro') {
            await this.processProModel(userMessage);
        }
        
        this.isGenerating = false;
        this.ui.updateSendButtonToSend();
    }
    // ==================== MODELO MISTRAL (codestral-latest) ====================
    async processMistralModel(userMessage) {
        // Usamos proxy server-side; n√£o √© obrigat√≥rio ter chave no localStorage para o deploy no Vercel
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();
        this.ui.setThinkingHeader('Processando com Mistral (codestral-latest)...', messageContainer.headerId);
        await this.ui.sleep(800);
        this.addToHistory('user', userMessage);
        try {
            // Gerar checks antes da chamada Mistral para mostrar racioc√≠nio tamb√©m neste fluxo
            const thinkingChecks = await this.generateChecksSafely(userMessage);
            for (let i = 0; i < thinkingChecks.length; i++) {
                const stepId = `step_${timestamp}_${i}`;
                const checkText = thinkingChecks[i].step;
                this.ui.addThinkingStep('schedule', checkText, stepId, messageContainer.stepsId);
                const delay = 800 + Math.random() * 1200;
                await this.ui.sleep(delay);
                this.ui.updateThinkingStep(stepId, 'check_circle', checkText);
                await this.ui.sleep(200);
            }

            let systemPrompt = {
                role: 'system',
                content: 'Voc√™ √© o Lhama Code 1, um assistente de c√≥digo inteligente. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com: m√∫ltiplos par√°grafos bem organizados, **palavras em negrito** para destacar conceitos, listas com ‚Ä¢ ou n√∫meros, t√≥picos claros com headings, e quando apropriado use tabelas (em formato markdown), nota√ß√£o matem√°tica (com $s√≠mbolos$ para inline ou $$blocos$$), e diagramas em ASCII. Evite blocos enormes de c√≥digo - prefira explica√ß√µes visuais. Seja t√©cnico mas acess√≠vel.'
            };
            const messages = this.extraMessagesForNextCall ? [systemPrompt, ...this.extraMessagesForNextCall, ...this.conversationHistory] : [systemPrompt, ...this.conversationHistory];

            // Chamamos o proxy server-side para Mistral (usar MISTRAL_API_KEY no servidor)
            let response = await this.callMistralAPI('codestral-latest', messages);
            this.extraMessagesForNextCall = null;

            if (!response || typeof response !== 'string') {
                throw new Error('Resposta vazia ou inv√°lida do servidor Mistral');
            }

            // Tentar extrair arquivos gerados na resposta e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    // Remover o bloco de arquivos do texto antes de exibir para usu√°rio
                    response = response.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta Mistral:', e);
            }

            // Armazenar e salvar a mensagem do assistente (incluindo attachments, se houver) ANTES de renderizar para que o UI possa detect√°-los
            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                const toPush = { role: 'assistant', content: response, thinking: null };
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) toPush.attachments = parsedFiles;
                chat.messages.push(toPush);
                this.ui.saveCurrentChat();
            }

            this.addToHistory('assistant', response);
            this.ui.setResponseText(response, messageContainer.responseId);
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);
        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem na API Mistral. ' + error.message, messageContainer.responseId);
            console.error('Erro no Modelo Mistral:', error);
        }
    }

    async callMistralAPI(model, messages) {
        // Usa proxy server-side /api/mistral-proxy
        this.abortController = new AbortController();
        try {
            const response = await fetch('/api/mistral-proxy', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ model, messages, temperature: 0.7, max_tokens: 2048 }),
                signal: this.abortController.signal
            });

            if (!response.ok) {
                const status = response.status;
                const text = await response.text().catch(() => null);
                if (status === 500 && text && text.includes('MISTRAL_API_KEY is not configured')) {
                    throw new Error('Mistral API Key n√£o est√° configurada no servidor. Adicione MISTRAL_API_KEY nas Environment Variables do Vercel.');
                }
                if (status === 401) {
                    throw new Error('Invalid API Key Mistral: verifique sua chave no Vercel para MISTRAL_API_KEY');
                }
                throw new Error(text || `Erro HTTP ${status}`);
            }

            const data = await response.json();
            return data.choices[0].message.content;
        } catch (error) {
            if (error.name === 'AbortError') {
                throw new Error('ABORTED');
            }
            throw error;
        }
    }

    stopGeneration() {
        console.log('üõë Parando gera√ß√£o...');
        this.isGenerating = false;
        if (this.abortController) {
            this.abortController.abort();
        }
        this.ui.showInterruptedMessage();
        this.ui.updateSendButtonToSend();
    }

    // Tenta extrair bloco ---FILES-JSON--- ... ---END-FILES-JSON--- e retornar array de arquivos
    parseFilesFromText(text) {
        try {
            const m = text.match(/---FILES-JSON---\s*([\s\S]*?)\s*---END-FILES-JSON---/i);
            if (!m) return null;
            const parsed = JSON.parse(m[1]);
            if (parsed && Array.isArray(parsed.files)) return parsed.files;
            return null;
        } catch (e) {
            console.warn('‚ö†Ô∏è Falha ao parsear blocos de arquivos:', e);
            return null;
        }
    }

    // Gera checks chamando Groq com toler√¢ncia a falhas
    async generateChecksSafely(userMessage) {
        try {
            const checksResponse = await this.callGroqAPI('llama-3.1-8b-instant', [
                {
                    role: 'system',
                    content: 'Voc√™ √© um gerador de checklist de pensamento. Baseado na pergunta/tarefa do usu√°rio, gere de 3 a 10 etapas de pensamento que uma IA deveria fazer para responder bem. Retorne APENAS um JSON array com objetos {step: "texto da etapa"}. Exemplo: [{"step": "Analisando a pergunta"}, {"step": "Consultando dados"}]'
                },
                {
                    role: 'user',
                    content: `Gere os passos de pensamento para esta tarefa: "${userMessage.substring(0, 200)}"`
                }
            ]);

            // Tentar parse tolerante
            let jsonText = null;
            const arrayMatch = checksResponse.match(/\[[\s\S]*?\]/);
            const fencedJsonMatch = checksResponse.match(/```json\s*([\s\S]*?)```/i) || checksResponse.match(/```\s*([\s\S]*?)```/);
            if (arrayMatch) {
                jsonText = arrayMatch[0];
            } else if (fencedJsonMatch) {
                jsonText = fencedJsonMatch[1];
            }

            if (jsonText) {
                return JSON.parse(jsonText);
            }

            const lines = checksResponse.split(/\r?\n/).map(l => l.trim()).filter(Boolean);
            const listItems = lines.filter(l => /^(\-|\*|\d+\.)\s+/.test(l)).map(l => {
                const s = l.replace(/^(\-|\*|\d+\.)\s+/, '');
                return { step: s };
            });
            if (listItems.length > 0) return listItems;

            // fallback
            return [
                { step: 'Analisando a pergunta' },
                { step: 'Consultando modelo Llama 3' },
                { step: 'Processando dados' },
                { step: 'Estruturando resposta' }
            ];
        } catch (e) {
            console.warn('‚ö†Ô∏è Erro ao gerar checks, usando padr√£o:', e);
            return [
                { step: 'Analisando a pergunta' },
                { step: 'Consultando modelo Llama 3' },
                { step: 'Processando dados' },
                { step: 'Estruturando resposta' }
            ];
        }
    }

    // Anexa arquivos parseados ao objeto de chat para reutiliza√ß√£o em chamadas futuras
    attachGeneratedFilesToChat(files) {
        try {
            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (!chat) return;
            chat.generatedFiles = chat.generatedFiles || [];
            // substituir arquivos com mesmo nome
            files.forEach(f => {
                const idx = chat.generatedFiles.findIndex(x => x.name === f.name);
                if (idx >= 0) chat.generatedFiles[idx] = f; else chat.generatedFiles.push(f);
            });
            this.ui.saveCurrentChat();
        } catch (e) {
            console.warn('‚ö†Ô∏è Falha ao anexar arquivos ao chat:', e);
        }
    }
    // parseFilesFromMessage removed (attachment parsing disabled)

    // ==================== MODELO R√ÅPIDO ====================
    async processRapidoModel(userMessage) {
        // Usamos proxy server-side (/api/groq-proxy) que utiliza GROQ_API_KEY em Vercel.
        // N√£o √© necess√°rio ter chave no localStorage para deploy em produ√ß√£o.

        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();

        this.ui.setThinkingHeader('Processando sua solicita√ß√£o de forma r√°pida...', messageContainer.headerId);
        await this.ui.sleep(800);

        const stepId = `step_${timestamp}`;
        this.ui.addThinkingStep('flash_on', 'Consultando Groq Llama 3.1 8B Instant', stepId, messageContainer.stepsId);
        await this.ui.sleep(1500);

        this.addToHistory('user', userMessage);

        try {
            const messages = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('rapido') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory
            ] : undefined;
            let response = await this.callGroqAPI('llama-3.1-8b-instant', messages);
            // limpar extras para pr√≥xima chamada
            this.extraMessagesForNextCall = null;
            this.ui.updateThinkingStep(stepId, 'check_circle', 'Resposta gerada com sucesso');
            await this.ui.sleep(500);

            this.addToHistory('assistant', response);
            this.ui.setResponseText(response, messageContainer.responseId);
            
            // Fechar racioc√≠nio quando terminar
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);

            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                chat.messages.push({ role: 'assistant', content: response, thinking: null });
                this.ui.saveCurrentChat();
            }

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.updateThinkingStep(stepId, 'error', 'Erro ao processar');
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem. ' + error.message, messageContainer.responseId);
            console.error('Erro no Modelo R√°pido:', error);
        }
    }

    // ==================== MODELO RACIOC√çNIO ====================
    async processRaciocioModel(userMessage) {
        // Usamos proxy server-side (/api/groq-proxy) que utiliza GROQ_API_KEY em Vercel.
        // N√£o √© necess√°rio ter chave no localStorage para deploy em produ√ß√£o.

        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();

        this.ui.setThinkingHeader('Entendi sua solicita√ß√£o, estou processando...', messageContainer.headerId);
        await this.ui.sleep(1200);

        // PRIMEIRA ETAPA: Gerar checks personalizados via IA econ√¥mica
        console.log('üîÑ Gerando checks personalizados...');
        
        let thinkingChecks = await this.generateChecksSafely(userMessage);
        
        // Mostrar checks gerados
        for (let i = 0; i < thinkingChecks.length; i++) {
            const stepId = `step_${timestamp}_${i}`;
            const checkText = thinkingChecks[i].step;
            
            this.ui.addThinkingStep('schedule', checkText, stepId, messageContainer.stepsId);
            
            // Delay vari√°vel entre checks para parecer mais natural
            const delay = 1500 + Math.random() * 1500;
            await this.ui.sleep(delay);
            
            this.ui.updateThinkingStep(stepId, 'check_circle', checkText);
            await this.ui.sleep(300);
        }

        this.addToHistory('user', userMessage);

        try {
            // SEGUNDA ETAPA: Gerar resposta com modelo principal (Mistral - codestral-latest)
            let systemPrompt;
            systemPrompt = { role: 'system', content: this.getSystemPrompt(this.currentModel) };
            const messages = this.extraMessagesForNextCall ? [systemPrompt, ...this.extraMessagesForNextCall, ...this.conversationHistory] : undefined;
            // Usar um modelo Groq de racioc√≠nio por padr√£o
            const modelName = 'llama-3.3-70b-versatile';
            console.log('üß≠ Usando modelo Groq:', modelName);
            let response = await this.callGroqAPI(modelName, messages);
            this.extraMessagesForNextCall = null;
            
            // Tentar extrair arquivos gerados na resposta e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    // Remover o bloco de arquivos do texto antes de exibir para usu√°rio
                    response = response.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta Groq:', e);
            }

            // Armazenar e salvar a mensagem do assistente (incluindo attachments, se houver) ANTES de renderizar para que o UI possa detect√°-los
            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                const toPush = { role: 'assistant', content: response, thinking: null };
                const parsedFiles = this.parseFilesFromText(response);
                if (parsedFiles && parsedFiles.length > 0) toPush.attachments = parsedFiles;
                chat.messages.push(toPush);
                this.ui.saveCurrentChat();
            }

            this.addToHistory('assistant', response);
            this.ui.setResponseText(response, messageContainer.responseId);
            
            // Fechar racioc√≠nio quando terminar
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem. Verifique sua API Key e tente novamente.', messageContainer.responseId);
            console.error('Erro no Modelo Racioc√≠nio:', error);
        }
    }

    // ==================== MODELO PRO ====================
    // 3 modelos Groq em 5 rounds + sintetizador
    async processProModel(userMessage) {
        // Usamos proxy server-side (/api/groq-proxy) que utiliza GROQ_API_KEY em Vercel.
        // N√£o √© necess√°rio ter chave no localStorage para deploy em produ√ß√£o.

        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();

        this.ui.setThinkingHeader('üöÄ Analisando com m√∫ltiplas perspectivas...', messageContainer.headerId);
        await this.ui.sleep(800);

        this.addToHistory('user', userMessage);

        try {
            // ========== ROUND 1: An√°lise paralela de 2 perspectivas ==========
            const step1aId = `step1a_${timestamp}`;
            this.ui.addThinkingStep('psychology', 'Perspectiva 1: An√°lise R√°pida', step1aId, messageContainer.stepsId);
            const messages1 = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('rapido') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory
            ] : undefined;
            const resp1Promise = this.callGroqAPI('llama-3.1-8b-instant', messages1);

            const step1bId = `step1b_${timestamp}`;
            this.ui.addThinkingStep('psychology', 'Perspectiva 2: An√°lise Profunda', step1bId, messageContainer.stepsId);
            const messages2 = this.extraMessagesForNextCall ? [
                { role: 'system', content: this.getSystemPrompt('raciocinio') },
                ...this.extraMessagesForNextCall,
                ...this.conversationHistory
            ] : undefined;
            const resp2Promise = this.callGroqAPI('llama-3.3-70b-versatile', messages2);

            // Esperar ambas em paralelo
            let [resp1, resp2] = await Promise.all([resp1Promise, resp2Promise]);
            // Extrair arquivos se existirem e remover do texto para n√£o expor JSON no chat
            try {
                const parsed1 = this.parseFilesFromText(resp1);
                if (parsed1 && parsed1.length > 0) {
                    this.attachGeneratedFilesToChat(parsed1);
                    resp1 = resp1.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) { console.warn('‚ö†Ô∏è Falha parsing arquivos de resp1:', e); }
            try {
                const parsed2 = this.parseFilesFromText(resp2);
                if (parsed2 && parsed2.length > 0) {
                    this.attachGeneratedFilesToChat(parsed2);
                    resp2 = resp2.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) { console.warn('‚ö†Ô∏è Falha parsing arquivos de resp2:', e); }
            
            this.ui.updateThinkingStep(step1aId, 'check_circle', '‚úÖ Perspectiva 1');
            this.ui.updateThinkingStep(step1bId, 'check_circle', '‚úÖ Perspectiva 2');
            await this.ui.sleep(1200);

            // ========== ROUND 2: Cross-review - cada perspectiva valida a outra ==========
            const step2aId = `step2a_${timestamp}`;
            this.ui.addThinkingStep('compare_arrows', 'Review 1‚Üí2: Valida√ß√£o Cruzada', step2aId, messageContainer.stepsId);
            
            const review1Promise = this.callGroqAPI('llama-3.1-8b-instant', [
                {
                    role: 'system',
                    content: 'Voc√™ √© um revisor cr√≠tico. Avalie a resposta de outro modelo e identifique: 1) O que est√° certo, 2) O que poderia melhorar, 3) Detalhes que faltam. Seja breve e direto.'
                },
                {
                    role: 'user',
                    content: `Pergunta original: "${userMessage}"\n\nResposta a revisar:\n${resp2}\n\nFa√ßa uma revis√£o cr√≠tica breve.`
                }
            ]);

            const step2bId = `step2b_${timestamp}`;
            this.ui.addThinkingStep('compare_arrows', 'Review 2‚Üí1: Valida√ß√£o Cruzada', step2bId, messageContainer.stepsId);
            
            const review2Promise = this.callGroqAPI('llama-3.3-70b-versatile', [
                {
                    role: 'system',
                    content: 'Voc√™ √© um revisor cr√≠tico. Avalie a resposta de outro modelo e identifique: 1) O que est√° certo, 2) O que poderia melhorar, 3) Detalhes que faltam. Seja breve e direto.'
                },
                {
                    role: 'user',
                    content: `Pergunta original: "${userMessage}"\n\nResposta a revisar:\n${resp1}\n\nFa√ßa uma revis√£o cr√≠tica breve.`
                }
            ]);

            const [review1, review2] = await Promise.all([review1Promise, review2Promise]);
            
            this.ui.updateThinkingStep(step2aId, 'check_circle', '‚úÖ Review 1‚Üí2');
            this.ui.updateThinkingStep(step2bId, 'check_circle', '‚úÖ Review 2‚Üí1');
            await this.ui.sleep(1200);

            // ========== S√çNTESE: Consolidar em resposta final ==========
            const stepSynthId = `stepsynth_${timestamp}`;
            this.ui.addThinkingStep('build', 'Consolida√ß√£o: S√≠ntese Final', stepSynthId, messageContainer.stepsId);
            
            const synthMessages = [
                {
                    role: 'system',
                    content: this.getSystemPrompt('pro') + ' Voc√™ √© um sintetizador especializado. Sua √öNICA fun√ß√£o √© consolidar duas an√°lises completas em UMA √öNICA resposta final coerente e equilibrada. Inclua os melhores pontos de ambas as perspectivas. N√ÉO adicione informa√ß√µes novas.'
                },
                {
                    role: 'user',
                    content: `Pergunta original: "${userMessage}"\n\n=== PERSPECTIVA 1 (R√°pida) ===\n${resp1}\n\n=== PERSPECTIVA 2 (Profunda) ===\n${resp2}\n\n=== FEEDBACK CRUZADO ===\nReview de 2 sobre 1: ${review1}\nReview de 1 sobre 2: ${review2}\n\nAgora, CONSOLIDE tudo em UMA resposta final √∫nica, equilibrada e bem estruturada.`
                }
            ];
            // Se houver arquivos anexados, inclu√≠-los temporariamente nas mensagens de s√≠ntese
            if (this.extraMessagesForNextCall) {
                synthMessages.splice(1, 0, ...this.extraMessagesForNextCall);
            }
            let finalResponse = await this.callGroqAPI('llama-3.1-8b-instant', synthMessages);
            this.extraMessagesForNextCall = null;
            this.ui.updateThinkingStep(stepSynthId, 'check_circle', '‚úÖ S√≠ntese Conclu√≠da');

            

            // Tentar extrair arquivos gerados na resposta de s√≠ntese e anex√°-los ao chat
            try {
                const parsedFiles = this.parseFilesFromText(finalResponse);
                if (parsedFiles && parsedFiles.length > 0) {
                    this.attachGeneratedFilesToChat(parsedFiles);
                    // remover bloco do texto para apresenta√ß√£o
                    finalResponse = finalResponse.replace(/---FILES-JSON---[\s\S]*?---END-FILES-JSON---/i, '').trim();
                }
            } catch (e) {
                console.warn('‚ö†Ô∏è Falha parsing arquivos de resposta (Pro):', e);
            }

            this.addToHistory('assistant', finalResponse);
            this.ui.setResponseText(finalResponse, messageContainer.responseId);
            
            // Fechar racioc√≠nio quando terminar
            await this.ui.sleep(500);
            this.ui.closeThinkingSteps(messageContainer.headerId);

            const chat = this.ui.chats.find(c => c.id === this.ui.currentChatId);
            if (chat) {
                if (chat.messages.length === 1) {
                    const firstUserMessage = chat.messages[0].content;
                    chat.title = firstUserMessage.substring(0, 50) + (firstUserMessage.length > 50 ? '...' : '');
                }
                const toPush = { role: 'assistant', content: finalResponse, thinking: null };
                const parsedFiles = this.parseFilesFromText(finalResponse);
                if (parsedFiles && parsedFiles.length > 0) toPush.attachments = parsedFiles;
                chat.messages.push(toPush);
                this.ui.saveCurrentChat();
            }

            console.log('üéâ Modelo Pro conclu√≠do com sucesso!');

        } catch (error) {
            if (error.message === 'ABORTED') {
                console.log('‚ö†Ô∏è Gera√ß√£o interrompida pelo usu√°rio');
                return;
            }
            console.error('Erro no Modelo Pro:', error);
            this.ui.setResponseText('Desculpe, ocorreu um erro ao processar sua mensagem no modo Pro. ' + error.message, messageContainer.responseId);
        }
    }

    // ==================== APIS ====================
    // Gemini API methods removed (attachments/Gemini integration disabled)

async callGroqAPI(model, customMessages = null) {
    // Not required to have a client-side Groq API key when using server-side proxy
    // The proxy will use GROQ_API_KEY from environment variables on Vercel
    
    // System prompts diferenciados por modelo
        let systemPrompt;
        if (this.currentModel === 'rapido') {
            systemPrompt = {
                role: 'system',
                content: 'Voc√™ √© o Lhama Code 1, um assistente de c√≥digo r√°pido e direto. Mantenha as respostas BREVES e CONCISAS - m√°ximo 2-3 par√°grafos. Evite elabora√ß√µes desnecess√°rias. V√° direto ao ponto.'
            };
        } else {
            // Racioc√≠nio e Pro - respostas ricas
            systemPrompt = {
                role: 'system',
                content: 'Voc√™ √© o Lhama Code 1, um assistente de c√≥digo inteligente. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com: m√∫ltiplos par√°grafos bem organizados, **palavras em negrito** para destacar conceitos, listas com ‚Ä¢ ou n√∫meros, t√≥picos claros com headings, e quando apropriado use tabelas (em formato markdown), nota√ß√£o matem√°tica (com $s√≠mbolos$ para inline ou $$blocos$$), e diagramas em ASCII. Evite blocos enormes de c√≥digo - prefira explica√ß√µes visuais. Seja t√©cnico mas acess√≠vel.'
            };
        }

        const messages = customMessages || [systemPrompt, ...this.conversationHistory];

        // Criar novo AbortController para cada requisi√ß√£o
        this.abortController = new AbortController();

        try {
            // Chamar proxy server-side no Vercel
            const response = await fetch('/api/groq-proxy', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ model, messages, temperature: 0.7, max_tokens: 8192, top_p: 1, stream: false }),
                signal: this.abortController.signal
            });

            if (!response.ok) {
                const status = response.status;
                const text = await response.text().catch(() => null);
                // Mensagens amig√°veis para erros comuns
                if (status === 500 && text && text.includes('GROQ_API_KEY is not configured')) {
                    throw new Error('GROQ API Key n√£o est√° configurada no servidor. Adicione GROQ_API_KEY nas Environment Variables do Vercel.');
                }
                if (status === 401) {
                    throw new Error('Invalid API Key: Verifique sua chave no Vercel para GROQ_API_KEY.');
                }
                throw new Error(text || `Erro HTTP ${status}`);
            }

            const data = await response.json().catch(() => ({}));

            // Normalizar formatos comuns de resposta de proxies/LLMs
            let content = null;
            if (typeof data.content === 'string') {
                content = data.content;
            } else if (data.choices && Array.isArray(data.choices) && data.choices[0]) {
                const choice = data.choices[0];
                if (choice.message && typeof choice.message.content === 'string') {
                    content = choice.message.content;
                } else if (typeof choice.text === 'string') {
                    content = choice.text;
                }
            } else if (typeof data === 'string') {
                content = data;
            }

            if (!content || typeof content !== 'string' || content.trim().length === 0) {
                console.error('[callGroqAPI] resposta inesperada do proxy:', data);
                throw new Error('Resposta vazia ou formato inesperado do proxy Groq');
            }

            return content;
        } catch (error) {
            if (error.name === 'AbortError') {
                console.log('‚ö†Ô∏è Requisi√ß√£o foi abortada pelo usu√°rio');
                throw new Error('ABORTED');
            }
            throw error;
        }
    }

    // ==================== UTILITIES ====================
    addToHistory(role, content) {
        this.conversationHistory.push({
            role: role,
            content: content
        });

        if (this.conversationHistory.length > this.maxHistoryMessages) {
            this.conversationHistory.shift();
            console.log('üóëÔ∏è Mensagem mais antiga removida (limite de 10 mensagens)');
        }
    }

    // Retorna o system prompt apropriado por 'mode' para estabelecer tom/estilo (inclui emojis)
    getSystemPrompt(mode) {
        switch (mode) {
            case 'rapido':
                return 'Voc√™ √© o Lhama Code 1, um assistente gentil, ador√°vel e otimista üòä. Use um tom caloroso e amig√°vel, inclua emojis com leveza para refor√ßar emo√ß√µes, e mantenha as respostas BREVES e objetivas (2-3 par√°grafos m√°ximo). Seja educado, encorajador e pr√°tico.';
            case 'raciocinio':
                return 'Voc√™ √© o Lhama Code 1, um assistente t√©cnico e claro üôÇ. Use emojis de forma moderada para tornar o texto mais acess√≠vel. Forne√ßa respostas COMPLETAS e ESTRUTURADAS com exemplos e explica√ß√µes claras.';
            case 'pro':
                return 'Voc√™ √© o Lhama Code 1, um assistente profissional e formal üßë‚Äçüíº. Use linguagem precisa e formal; inclua emojis pontualmente para dar tom (com parcim√¥nia). Forne√ßa an√°lises detalhadas, recomenda√ß√µes e justificativas bem fundamentadas.';
            default:
                return 'Voc√™ √© o Lhama Code 1, um assistente de c√≥digo. Forne√ßa respostas claras e √∫teis, com boa estrutura e exemplos quando adequado.';
        }
    }

    showError(message) {
        const messageContainer = this.ui.createAssistantMessageContainer();
        const timestamp = Date.now();
        
        const errorStepId = `errorStep_${timestamp}`;
        this.ui.addThinkingStep('error', 'Erro detectado', errorStepId, messageContainer.stepsId);
        
        this.ui.setResponseText(message, messageContainer.responseId);
        
        console.error(message);
    }

    async test() {
        console.log('üß™ Iniciando teste do agente...');
        
        console.log('üì° Testando conex√£o com Groq via proxy (server-side) ...');
        console.log('‚ÑπÔ∏è Se voc√™ configurou a vari√°vel GROQ_API_KEY no Vercel, este teste usar√° ela. Caso contr√°rio, o teste falhar√° com mensagem adequada.');

        try {
            const testMessage = 'Ol√°! Estou testando a conex√£o.';
            console.log(`üì§ Enviando: "${testMessage}"`);
            
            this.addToHistory('user', testMessage);
            const response = await this.callGroqAPI('llama-3.3-70b-versatile');
            this.addToHistory('assistant', response);
            
            console.log('‚úÖ Resposta recebida:');
            console.log(response);
            console.log('\nüéâ Teste conclu√≠do com sucesso!');
            console.log(`üìä Hist√≥rico: ${this.conversationHistory.length} mensagens`);
            
            return response;
        } catch (error) {
            console.error('‚ùå Erro no teste:', error.message);
            console.error('Detalhes:', error);
            return null;
        }
    }

    clearHistory() {
        this.conversationHistory = [];
        console.log('üóëÔ∏è Hist√≥rico de conversa limpo');
    }

    getHistoryStats() {
        const userMessages = this.conversationHistory.filter(m => m.role === 'user').length;
        const assistantMessages = this.conversationHistory.filter(m => m.role === 'assistant').length;
        
        console.log('üìä Estat√≠sticas do Hist√≥rico:');
        console.log(`   Total: ${this.conversationHistory.length} mensagens`);
        console.log(`   Suas mensagens: ${userMessages}`);
        console.log(`   Minhas respostas: ${assistantMessages}`);
        console.log(`   Limite m√°ximo: ${this.maxHistoryMessages} mensagens`);
        
        return {
            total: this.conversationHistory.length,
            user: userMessages,
            assistant: assistantMessages,
            max: this.maxHistoryMessages
        };
    }

    // Extra√ß√£o e retorno de arquivos removidos (download de arquivos pela IA desativado)

    sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }
}
